{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What to look for in regression model output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The best single error statistic to look at is the standard error of the regression, which is the estimated standard deviation of the unexplainable variations in the dependent variable. <br>\n",
    "This what the software is trying to minimize when estimating coefficients.\n",
    "2. Standard error of the regression -  is a lower bound on the standard error of any forecast generated from the model.<br>\n",
    "better look on it rather than the standard deviation of the errors.\n",
    "\n",
    "3. Adjusted R-squared - the fraction by which the variance of the errors is less than the variance of the dependent variable, one of the most overused, it just measure the MSE from the model of y=CONSTANT, which may or may not be the appropriate naive model for purposes of comparison(Better to determine the best naive model first).<br>\n",
    "If using MSE(minimizing $SS_{res}$). In this case, R2 increases as the number of variables in the model is increased (R2 is monotone increasing with the number of variables includedâ€”it will never decrease). This illustrates a drawback to one possible use of R2, where one might keep adding variables, to increase the $R^2$ value - alternative approach is Adjusted R-squared, which penalizes the as extra variables are included in the model.  \n",
    "\n",
    "4. Significance of the estimated coefficients - \n",
    "t-statistic - measures \"how many standard deviations from zero\" the estimated coefficient is, and it is used to test the hypothesis that the true value of the coefficient is non-zero, in order to confirm that the independent variable really belongs in the model. <br>\n",
    "in practice it usually turns out that a variable whose estimated coefficient has a p-value of greater than 0.05 can be dropped from the model without affecting the error measures very much.[1]\n",
    "\n",
    "5. Values of the estimated coefficients -\n",
    "The coefficient of a given independent variable is its proportional effect on the average value of the dependent variable, others things being equal. \n",
    "\n",
    "6. Plots of forecasts and residuals-  Do the forecasts \"track\" the data in a satisfactory way, apart from the inevitable regression-to-the mean?  Do the residuals appear random, or do you see some systematic patterns in their signs or magnitudes? is there heteroscedasticity? <br>\n",
    "If heteroscedasticity and/or non-normality is a problem, you may wish to consider a nonlinear transformation of the dependent variable, such as log\n",
    ". <br>\n",
    "You do not usually rank (i.e., choose among) models on the basis of their residual diagnostic tests, but bad residual diagnostics indicate that the model's error measures may be unreliable and that there are probably better models out there somewhere. \n",
    "7. Out-of-sample validation - a vlidation dataset, it is the best way to compare models, mke sure they are big enouth and and similar(as muxh as possible) to the train dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources:\n",
    "[1] https://people.duke.edu/~rnau/411regou.htm <br>\n",
    "[2] https://en.wikipedia.org/wiki/Coefficient_of_determination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
